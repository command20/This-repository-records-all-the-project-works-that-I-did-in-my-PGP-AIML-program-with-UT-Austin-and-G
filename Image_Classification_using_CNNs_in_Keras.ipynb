{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "01kgE9hgunni"
   },
   "source": [
    "# Image Classification using CNNs in Keras - Project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x1je_5iPunoB"
   },
   "source": [
    "# Contents:\n",
    "------------------------\n",
    "-  . <a href = #link100>Context:</a>\n",
    "- 1. <a href = #link1>Importing Libraries</a>\n",
    "- 2. <a href = #link2>Reading and Review of the dataset.</a>\n",
    "- 3. <a href = #link3>Transposing index and columns</a>\n",
    "- 4. <a href = #link4>EDA Discriptive Observations</a>\n",
    "- 5. <a href = #link5> Dataset splitting Train and Test.</a>\n",
    "- 6. <a href = #link6>One Hot encoding to target values..</a>\n",
    "- 7. <a href = #link7>Gussian Blurring.</a>\n",
    "- 8. <a href = #link8>Building the CNN a Model.</a>\n",
    "- 9. <a href = #link9>Model Evaluation.</a>\n",
    "- 10. <a href = #link10>Model Retraing.</a>\n",
    "- 11. <a href = #link11>Conclusion.</a>\n",
    "- 12. <a href = #link12>References & GitHub Link.</a>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4nnaXE6unoC"
   },
   "source": [
    "# <a id='link100'> Context:</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RvdZBaHcunoD"
   },
   "source": [
    "# Data Description:\n",
    "\n",
    "You are provided with a dataset of images of plant seedlings at various stages of grown. Each image has a filename that is its unique id. The dataset comprises 12 plant species. The goal of the project is to create a classifier capable of determining a plant's species from a photo.Dataset:\n",
    " \n",
    "**The dataset can be download from Olympus.**\n",
    "The data file names are:\n",
    "\n",
    "* images.npy\n",
    "* Label.csv\n",
    "\n",
    "The original files are from Kaggle. Due to the large volume of data, the images were converted to images.npy file and the labels are also put into the Labels.csv.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yR5q0xYunoE"
   },
   "source": [
    "Can you differentiate a weed from a crop seedling?\n",
    "The ability to do so effectively can mean better crop yields and better stewardship of the environment.The Aarhus University Signal Processing group, in collaboration with University of Southern Denmark, has recently released a dataset containing images of unique plants belonging to 12 species at several growth stages.\n",
    "\n",
    "**Objective:**\n",
    "\n",
    "To implement the techniques learnt as a part of the course.\n",
    "\n",
    "**Learning Outcomes:**\n",
    "*  Pre-processing of image data.\n",
    "*  Visualization of images.\n",
    "*  Building CNN.\n",
    "*  Evaluate the Model.\n",
    "*  The motive of the project is to make the learners capable to handle images/image classification  problems, during this process you should also be capable to handle real image files, not just limited  to a numpy array of image pixels.\n",
    "\n",
    "**Guide to solve the project seamlessly:**\n",
    "Here are the points which will help you to solve the problem efficiently:\n",
    "* Read the problem statement carefully from start to end (including the note at the end). The   highlighted part in the attached problem statement should not be missed.\n",
    "* Download the dataset from the Olympus platform.\n",
    "* Upload the \"images.npy\" and “Labels.csv” file to google drive.\n",
    "* Then you can use the dataset path in the Google Colab notebook to do further steps related to project problem statement.\n",
    "* You can set runtime type to “GPU” in Google Colab, so that the code will run faster as you will be using CNN to fit your model.\n",
    "\n",
    "**Steps and tasks:**\n",
    "1. Import the libraries, load dataset, print shape of data, visualize the images in dataset.\n",
    "2. Data Pre-processing:\n",
    "\n",
    "a. Normalization.\n",
    "b. Gaussian Blurring.\n",
    "c. Visualize data after pre-processing.\n",
    "\n",
    "3. Make data compatible: \n",
    "* a-Convert labels to one-hot-vectors.\n",
    "* b-Print the label for y_train[0].\n",
    "* c-Split the dataset into training, testing, and validation set.\n",
    "\n",
    "(Hint: First split images and labels into training and testing set with test_size = 0.3. Then further split test data into test and validation set with test_size = 0.5)\n",
    "* d- Check the shape of data, Reshape data into shapes compatible with Keras models if it’s not already. If it’s already in the compatible shape, then comment in the notebook that it’s already in compatible shape.\n",
    "\n",
    "4. Building CNN: \n",
    "* a. Define layers.\n",
    "* b. Set optimizer and loss function. (Use Adam optimizer and categorical crossentropy.)\n",
    "5. Fit and evaluate model and print confusion matrix. (10 Marks)\n",
    "6. Visualize predictions for x_test[2], x_test[3], x_test[33], x_test[36], x_test[59]. (5 Marks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7K9iOJECunoF"
   },
   "source": [
    "# Environment and Algorithms techniques.\n",
    "* cv2\n",
    "* numpy \n",
    "* pandas as pd\n",
    "* seaborn as sns\n",
    "* matplotlib import pyplot as plt\n",
    "* tensorflow as tf\n",
    "* sklearn.model_selection import train_test_split\n",
    "* keras\n",
    "* keras.models/Sequential\n",
    "* keras.layers/Dense\n",
    "* sklearn.metrics/accuracy_score\n",
    "* sklearn.metrics/confusion_matrix\n",
    "* sklearn.metrics/classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdxdszV6unoG"
   },
   "source": [
    "# <a id='link1'> Importing Libraries</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "f7iV3zAIho18",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Traceback (most recent call last):\n  File \"C:\\Users\\bmw85\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m   \u001b[1;31m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-429033f3101d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m from tensorflow.keras.layers import (\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmodule_util\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_module_util\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlazy_loader\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLazyLoader\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_LazyLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;31m# go/tf-wildcard-import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# pylint: disable=wildcard-import,g-bad-import-order,g-import-not-at-top\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_pywrap_tensorflow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[1;33m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[1;32m---> 83\u001b[1;33m   \u001b[1;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Users\\bmw85\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 64, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: A dynamic link library (DLL) initialization routine failed.\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."
     ]
    }
   ],
   "source": [
    "# The following line od codes to import necessary libraries.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math \n",
    "import cv2\n",
    "import os\n",
    "\n",
    "from glob import  glob\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (\n",
    "    Dense, \n",
    "    Dropout, \n",
    "    Flatten, \n",
    "    Conv2D, \n",
    "    MaxPooling2D, \n",
    "    MaxPool2D,\n",
    "    GlobalMaxPooling2D,\n",
    "    BatchNormalization\n",
    ")\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbhCKQ8HunoI"
   },
   "source": [
    "# <a id='link2'> Reading and Review of the dataset.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHNcb_F-unoI"
   },
   "source": [
    "## Loading the data.\n",
    "* We use panda's read_csv to read train.csv into a dataframe.\n",
    "* Then we separate our images and labels for supervised learning.\n",
    "* We also do a train_test_split to break our data into two sets, one for training and one for testing. This let's us measure how well our model was trained by later inputting some known test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zJVRkUiqvr0F"
   },
   "outputs": [],
   "source": [
    "# Set the path to the dataset folder. (The dataset contains image folder: \"train\")\n",
    "trainLabel = pd.read_csv('/content/drive/My Drive/Colab Notebooks/Labels.csv').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2OgkfWpuwFUI"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DHOdWCbunoK"
   },
   "outputs": [],
   "source": [
    "trainLabel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gqJTyrlZunoM"
   },
   "outputs": [],
   "source": [
    "# The following line of code to Load images file.\n",
    "trainImg= np.load('/content/drive/My Drive/Colab Notebooks/images.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zjIMf7mOunoN"
   },
   "outputs": [],
   "source": [
    "trainImg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nGM_mjlyunoO"
   },
   "source": [
    "# <a id='link3'>Transposing index and columns.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w-H7kRBJSP9L"
   },
   "outputs": [],
   "source": [
    "# The following line of codes to print array shape.\n",
    "print(f'Training image array shape:{trainImg.shape}')\n",
    "print(f'Training target labels:{trainLabel.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uo3LgdOlTjyf"
   },
   "outputs": [],
   "source": [
    "trainLabel.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dMDsmAGnunoP"
   },
   "source": [
    "# <a id='link4'> EDA Discriptive Observations.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ky-sMm6lyWUo"
   },
   "outputs": [],
   "source": [
    "# The following line of code is to randomize the order of training set.\n",
    "SEED = 42\n",
    "train_df = trainLabel.sample(frac=1, random_state=SEED)\n",
    "train_df.index = np.arange(len(trainLabel)) # This to reset indices\n",
    "train_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgAh7ofeT4us"
   },
   "outputs": [],
   "source": [
    "plt.hist(train_df['Label'])\n",
    "plt.title('Frequency Histogram of Species')\n",
    "plt.figure(figsize=(20,18))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lAtSPVtfVMb4"
   },
   "source": [
    "# Checking the dataset for error, duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rDHdcwfVn1w"
   },
   "outputs": [],
   "source": [
    "trainLabel.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KlaEmsZBV2OA"
   },
   "outputs": [],
   "source": [
    "pd.isnull(trainLabel).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zBR_QEt1WHLO"
   },
   "outputs": [],
   "source": [
    "trainLabel.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gp-N3DjjWY8R"
   },
   "outputs": [],
   "source": [
    "# The following code to check duplicates in the datasets.\n",
    "dupes = trainLabel.duplicated()\n",
    "sum(dupes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9OXCJGWcYRhl"
   },
   "source": [
    "# Pre-Processing.\n",
    "\n",
    "* Normalizing the data.\n",
    "* Tain image and testing image needs to be normalized to 0-1 by dividing the values by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOfNaYmtXUXm"
   },
   "outputs": [],
   "source": [
    "trainImg = trainImg.astype('float32')\n",
    "trainImg /=255\n",
    "# the following line of codes to check the nomalized data.\n",
    "print(f'Shape of the Train array:{trainImg.shape}')\n",
    "print(f'Minimum value in the Train Array:{trainImg.min()}')\n",
    "print(f'Maximum value in the Train Array:{trainImg.max()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jm366lR3unoT"
   },
   "source": [
    "# <a id='link5'>Spliting The Dataset.</a>\n",
    "## Spliting the dataset into training, testing, and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SH-_m1djFhhD"
   },
   "outputs": [],
   "source": [
    "# Step 1:\n",
    "# The following line of codes to split the xtrain dataset.\n",
    "x_train, x_test, y_train, y_test = train_test_split(trainImg, trainLabel, test_size=0.3, random_state=42)\n",
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onQpa5PGH0aI"
   },
   "outputs": [],
   "source": [
    "# Step 2:\n",
    "# The following line of codes to split the validation dataset.\n",
    "x_test, x_validation, y_test, y_validation = train_test_split(x_test, y_test, test_size=0.5, random_state=42)\n",
    "\n",
    "x_test.shape, x_validation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L6x_4eV5unoV"
   },
   "source": [
    "# <a id='link6'>One Hot encoding to target values.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMSFR8uLKWgZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import  LabelBinarizer\n",
    "encoder = LabelBinarizer()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.fit_transform(y_test)\n",
    "y_validation = encoder.fit_transform(y_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgaRswbnunoV"
   },
   "source": [
    "## Converting Target Variables.\n",
    "\n",
    "* Will convert the target variable to one-hot.\n",
    "* Will Convert String categorical to numeric.\n",
    "* Will use tensorflow.keras.utils.to_categorical to convert to binary array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2U_5P4zLh7M"
   },
   "outputs": [],
   "source": [
    "# The following line of codes to Print the label for y_train[0] target variable.\n",
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8uciCPcoVlZ"
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49fmjfSioCXJ"
   },
   "outputs": [],
   "source": [
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MjqIGe-L7ob"
   },
   "source": [
    "# <a id='link7'>Gussian Blurring.</a>\n",
    "Image blurring by convoliving the image with a low-pass filter kernel to be able to remove the noise also to remove high frequency content e.g (noise, edges) from the image resulting in edges being blurred when the filter is aplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KcVw2ngfNuxO"
   },
   "outputs": [],
   "source": [
    "# the following line of codes to preview the image before Gusian blur.\n",
    "plt.imshow(x_train[1], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQCDffApunoX"
   },
   "outputs": [],
   "source": [
    "# Preview the image before Gaussian Blur\n",
    "plt.imshow(x_train[1], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cwgZJ5d9OXgM"
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.imshow(cv2.GaussianBlur(x_train[1], (15,15), 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrShz4cDRn7W"
   },
   "outputs": [],
   "source": [
    "# the following line of codes to apply the gaussian blur to each 128x128 pixels array (image)\n",
    "# to reduce the noise in the the image.\n",
    "for idx, img in enumerate(x_train):\n",
    "  x_train[idx] = cv2.GaussianBlur(img, (5, 5), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ps1ieEfaSpn-"
   },
   "outputs": [],
   "source": [
    "# The following line of codes to preview the image after applying the Gaussian Blur.\n",
    "plt.imshow(x_train[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aC62EU-1Uopc"
   },
   "outputs": [],
   "source": [
    "# The following line of codes to apply Gaussian Blue to test and validation.\n",
    "for idx, img in enumerate(x_test):\n",
    "  x_test[idx] = cv2.GaussianBlur(img, (5, 5), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I5_QcyTcVZGV"
   },
   "outputs": [],
   "source": [
    "for idx, img in enumerate(x_validation):\n",
    "  x_validation[idx] = cv2.GaussianBlur(img, (5, 5), 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "imXjNgvEunoZ"
   },
   "source": [
    "# <a id='link8'>Building the CNN a Model.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rLTLz5cjV8nh"
   },
   "source": [
    "# Steps:\n",
    "* Intializing the CNN Classifier\n",
    "* Adding Convolution layer with 32 kernels of 3x3 shape.\n",
    "* Adding Maxpooling layer of size 2x2.\n",
    "* Flatten will be the inpue array.\n",
    "* Adding dense layer with relu activation function.\n",
    "* Dropout the probability.\n",
    "* Adding softmax dense layer as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-eOi04rmZWiq"
   },
   "outputs": [],
   "source": [
    "# The following line of codes to build the CNN model.\n",
    "def create_model(input_shape, num_classes):\n",
    "  # Initializing the CNN Classifer.\n",
    "  model = Sequential()\n",
    "\n",
    "  # Adding the convolution layer with 32 filters and 3 kernels.\n",
    "  model.add(Conv2D(32, (3,3), input_shape=input_shape, padding='same', activation=tf.nn.relu))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(rate=0.25))\n",
    "\n",
    "  # Adding convolution layer with 32 filters and 3 kernels.\n",
    "  model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation=tf.nn.relu))\n",
    "  model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation=tf.nn.relu))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(rate=0.25))\n",
    "\n",
    "  # Adding the Convolution layer with 32 filter and 3 kernels.\n",
    "  model.add(Conv2D(filters=32, kernel_size=3, padding='same', activation=tf.nn.relu))\n",
    "  model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "  model.add(Dropout(rate=0.25))\n",
    "\n",
    "  # Flatten the 2D array to 1D array.\n",
    "  model.add(Flatten())\n",
    "\n",
    "  # Creating a fully connected layers with 512 units.\n",
    "  model.add(Dense(512, activation=tf.nn.relu))\n",
    "  model.add(Dropout(0.5))\n",
    "\n",
    "  # Adding a fully connected layer with 128 neurons.\n",
    "  model.add(Dense(units = 128, activation = tf.nn.relu )) \n",
    "  model.add(Dropout(0.5))\n",
    "\n",
    "  # The final output layer with 12 neurons to predict the categorical classification.\n",
    "  model.add(Dense(units= num_classes, activation= tf.nn.softmax))\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeQdBNa5n8A3"
   },
   "source": [
    "**Model includes.**\n",
    "* **The Sequential** defines a sequence of layers.\n",
    "* **Conv2D:** Keras Conv2D is a 2D convolution layer, this creates a convolution kernel that is is wind with layers input which helps preduce a tensor of outputs.\n",
    "* **MaxPool2D:** The objective is to down-sample an input representation. \n",
    "* **Flatten:** Convert the 2D to 1D array. \n",
    "* **Dense:** adds a layers of neurons.\n",
    "\n",
    "* **Activation Functions:** \n",
    "* **Relu:** Relu effectively means \"if X>0return X, else return 0' -- so what it does it only passes values 0 or greater to the next layer in the network.\n",
    "* **Sofmax:** function is also a type of sigmoid function but is handy when we are trying to handle classification problems.\n",
    "* Usually used when trying to handle multiple classes. The softmax function would squeeze the outputs for each class between 0 and 1 and would also divide by the sum of the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h20-2JiH3aVf"
   },
   "outputs": [],
   "source": [
    "class mycallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('accuracy')>0.95):\n",
    "      print('\\nReached 95% accuracy so cancelling training!')\n",
    "      self.model.stop_training = True\n",
    "\n",
    "callbacks = mycallback()\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='min', verbose=1, patience=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGGBUpAa6eUM"
   },
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1:] #Input shape of xtrain\n",
    "num_classes = y_train.shape[1] # Target ccolumn size\n",
    "\n",
    "model = create_model(input_shape, num_classes)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) # Optimizer\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BZUxx99z-aiy"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_data=(x_validation, y_validation), epochs=30, batch_size=100, callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJYDIMNw0N5N"
   },
   "source": [
    "* The above score shows that the accuracy is good, as we used number of epochs = 30. If we use more epochs and tune the hyper-parameters more then we can get some more accuracy score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s-aIDwiaA82S"
   },
   "outputs": [],
   "source": [
    "# Plotting the history.\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel(r'Loss', fontsize=18)\n",
    "plt.legend(('loss train','loss validation'), loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05ndXLK7Fjwm"
   },
   "outputs": [],
   "source": [
    "# The following line of codes to plot the accuracy.\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.xlabel('Epoch', fontsize=18)\n",
    "plt.ylabel(r'Loss', fontsize=18)\n",
    "plt.legend(('accuracy train', 'accuracy validation'), loc=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nP2Tg8VLunof"
   },
   "source": [
    "# <a id='link9'>Model Evaluation.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IsdsRj7nG-s_"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('Test loss: {:.2f} \\n Test accuracy: {:.2f}'.format(loss, accuracy))\n",
    "\n",
    "loss, accuracy = model.evaluate(x_train, y_train)\n",
    "print('Train lodd: {:.2f} \\n Train accuracy: {:.2f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ryq7PZ7unof"
   },
   "source": [
    "* Model is overfitting since training accuracy is 97% and testing accuracy is 83%. let's stop it before 18 epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JA84E4IWunog"
   },
   "source": [
    "# <a id='link10'>Model Retraing.</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zzvhYhu5JSxl"
   },
   "outputs": [],
   "source": [
    "# The following line of codes to rint the summary report.\n",
    "model = create_model(input_shape, num_classes)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001) # Optimizer\n",
    "\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WdhjdKCDK_Si"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, validation_data=(x_validation, y_validation), epochs=18, callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E70e9WjaM3Cn"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test)\n",
    "print('Test loss: {:.2f} \\n Test accuracy: {:.2f}'.format(loss, accuracy))\n",
    "\n",
    "loss, accuracy = model.evaluate(x_train, y_train)\n",
    "print('Train loss: {:2f} \\n Train accuracy: {:.2f}'.format(loss, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nYunA5Rxwavo"
   },
   "source": [
    "\n",
    "* The early stopping helping model to balance accuracy b/w test and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cADBdgPLuVj4"
   },
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOosQbjTqnW7"
   },
   "outputs": [],
   "source": [
    "model.save('./CIFAR_classifier.h5')                     # save classifier (model) and architecture to single file\n",
    "\n",
    "model.save_weights('./CIFAR_classifier_weights.h5')     # weights are saved directly from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPQ8zEppunol"
   },
   "source": [
    "## Classification Report and Confusion matrix Plotting:\n",
    "Following line of code to rint a classification report and Confusion to measure the quality of predictions from a classification algorithm. it will indicate how many predictions are True and how many are not. it will point the True positives, and False positives, True negatives and false negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jL54gdVDPsiX"
   },
   "outputs": [],
   "source": [
    "# The following line of codes to rint the classification report.\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = model.predict(x_test)\n",
    "print(classification_report(y_test.astype('int'), y_pred.astype('int')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnGO6ftLZlD6"
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s4dKINkQZ6RL"
   },
   "outputs": [],
   "source": [
    "print('======== Classification Matrix ========')\n",
    "print(confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m48iJEIJbFr8"
   },
   "outputs": [],
   "source": [
    "print('=========== Classification Report ====================')\n",
    "print(classification_report(y_test.argmax(axis=1), y_pred.argmax(axis=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GanHvLGAunoo"
   },
   "source": [
    "## Observation.\n",
    "\n",
    "* **Precision:** Out of all the positive classes we have predicted correctly, how many are actually positive.\n",
    "\n",
    "* **Recall:** Out of all the positive classes, how much we predicted correctly. It should be high as possible.\n",
    "\n",
    "* **F1-Score:** F1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. Intuitively it is not as easy to understand as accuracy, but F1 is usually more useful than accuracy, especially if you have an uneven class distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zl6rdKnSunoo"
   },
   "source": [
    "## Visualize Predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HB4F8K_GcRzm"
   },
   "outputs": [],
   "source": [
    "y_pred = encoder.inverse_transform(y_pred)\n",
    "\n",
    "index = 2\n",
    "plt.imshow(x_test[index], cmap='gray')\n",
    "print('Predicted Label:', y_pred[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m03VOgRCdCli"
   },
   "outputs": [],
   "source": [
    "index = 3\n",
    "plt.imshow(x_test[index], cmap='gray')\n",
    "print('Predicted Label:', y_pred[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C_vSfojydl32"
   },
   "outputs": [],
   "source": [
    "index = 33\n",
    "plt.imshow(x_test[index], cmap='gray')\n",
    "print('Predicted Label:', y_pred[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z2bwSFdheZCu"
   },
   "outputs": [],
   "source": [
    "index = 36\n",
    "plt.imshow(x_test[index], cmap='gray')\n",
    "print('Predicted Lable:', y_pred[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZSE5yWie-D6"
   },
   "outputs": [],
   "source": [
    "index = 59\n",
    "plt.imshow(x_test[index], cmap='gray')\n",
    "print('Predicted Label:', y_pred[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zANXHoeFunor"
   },
   "source": [
    "# <a id='link11'>Conclusion.</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DmUhxHMhX3sJ"
   },
   "source": [
    "The purpose of this case study is to make a prediction to differentiate a weed from a crop seeding. the ability to do so effectively can mean better crop yields and better stewardship of the environment. \n",
    "The Aarhus university Sigal processing group, in collaboration with the University of Southern Denmark, has recently released a dataset containing images of unique plants belonging to 12 species at several growth stages.\n",
    "\n",
    "We presented a deep learning-based approach for plant image prediction which accurately predicted to differentiate a weed from crop seeding yield the entire cord belt.\n",
    "Most importantly, our methodology moved beyond prediction as it provided key results towards explaining yield prediction variable importance by time period.\n",
    "\n",
    "By implementing different CNN deep machine learning techniques. The solution by building a CNN model that has ab accuracy of approximately from 80% to 85% prediction capability with a high percentage of accuracy also more improvements can be made to more improve it.\n",
    "\n",
    "by visualizing the results in order to check which class has the best and worst performance and further necessary parameters tunning steps can be taken in order to improve the results, seek the accuracy rate in order to find the validation accuracies and losses converge nicely.\n",
    "\n",
    "The model F1 score results within the validation accuracy of 84% and an error loss rate of approximately 13%.\n",
    "\n",
    "In this case study we discovered how to create deep CNNs in Keras for image classification. After working through this case study we learned:\n",
    "\n",
    "* About the CIFAR-10 dataset and how to load it in Keras and plot examples from the dataset.\n",
    "* How to train and evaluate a Convolutional Neural Network on the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KuKJgg3bunos"
   },
   "source": [
    "# <a id='link12'>References & GitHub Link..</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "It8dnLpoMPZW"
   },
   "source": [
    "* https://github.com/command20/This-repository-records-all-the-project-works-that-I-did-in-my-PGP-AIML-program-with-UT-Austin-and-G\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oj-_XUfZDktK"
   },
   "source": [
    "* https://machinelearningmastery.com/display-deep-learning-model-training-history-in-keras/\n",
    "* https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/\n",
    "* https://keras.io/api/callbacks/\n",
    "* https://towardsdatascience.com/3-ways-to-build-neural-networks-in-tensorflow-with-the-keras-api-80e92d3b5b7e\n",
    "* https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/\n",
    "* https://www.tensorflow.org/guide/function#python_or_tensor_args\n",
    "* https://www.tensorflow.org/api_docs/python/tf/function\n",
    "* https://developer.nvidia.com/cuda-gpus\n",
    "* https://towardsdatascience.com/tensorflow-gpu-installation-made-easy-use-conda-instead-of-pip-52e5249374bc\n",
    "* https://www.tensorflow.org/tutorials/images/classification\n",
    "* Xing E., Jordan M., Karp R, “Feature selection for high￾dimensional genomic microarray data,” Proceedings of the Eighteenth International Conference on Machine Learning, Massachusetts, USA: Morgan Kaufmann, 2001, 601–608.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "GanHvLGAunoo"
   ],
   "name": "Image Classification using CNNs in Keras.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
